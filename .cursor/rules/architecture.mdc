---
description: 
globs: 
alwaysApply: true
---
# AI-Powered Observability Demo Generator: Complete Architecture & Implementation Guide

## Project Overview & Mission

**Mission Statement**: Generate realistic, comprehensive telemetry data (traces, logs, metrics) for user-defined microservices scenarios through AI-powered configuration generation and sophisticated synthetic data simulation.

**Core Value Proposition**: 
- **Zero Infrastructure**: No need to deploy actual microservices for observability demos
- **AI-Driven**: Natural language → production-ready observability scenarios  
- **Maximum Realism**: Multi-language services, realistic performance patterns, semantic conventions
- **Instant Setup**: From description to streaming telemetry in seconds

**Target Use Cases**:
- Observability platform demos and training
- Testing observability tooling and alerting
- Performance pattern simulation (N+1 queries, latency spikes, error cascades)
- Multi-language polyglot environment simulation
- Customer proof-of-concepts for observability vendors

## Current Implementation Status ✅

### ✅ Completed Components
- **FastAPI Backend**: Full REST API with telemetry generation engine
- **React/Vite Frontend**: Modern UI with Tailwind CSS styling
- **OpenAI Integration**: o3 model for YAML config generation
- **OTLP JSON Generation**: Complete traces, logs, metrics with semantic conventions
- **Multi-language Simulation**: Runtime-specific metrics per language
- **Sophisticated Config Schema**: Operations, latency configs, example queries
- **Local Development Environment**: Automated startup scripts

### ✅ Working Features
- Natural language → YAML configuration generation
- Real-time telemetry streaming to OTLP collectors
- Comprehensive error injection and performance patterns
- Resource attribute simulation for polyglot environments
- Business operation modeling with realistic database queries
- Probabilistic latency injection for performance testing

## System Architecture

### High-Level Data Flow
```
User Input (Natural Language) 
    ↓ 
OpenAI o3 Model 
    ↓ 
YAML Configuration 
    ↓ 
Telemetry Generation Engine 
    ↓ 
OTLP JSON Payloads 
    ↓ 
OpenTelemetry Collector 
    ↓ 
Observability Backend (Elastic, Jaeger, etc.)
```

### Component Architecture

#### Backend Service (Python/FastAPI)
**File Structure**:
```
backend/
├── main.py              # FastAPI app, CORS, API endpoints
├── generator.py         # Core telemetry generation engine  
├── llm_config_gen.py    # OpenAI integration & system prompts
├── config_schema.py     # Pydantic models & validation
└── requirements.txt     # Dependencies
```

**API Endpoints**:
- `POST /generate-config`: Natural language → YAML config
- `POST /start`: Start telemetry generation with config
- `POST /stop`: Stop telemetry generation
- `GET /status`: Get generator status and current config
- `GET /`: Health check

**Key Classes & Responsibilities**:
- `TelemetryGenerator`: Main orchestration, threading, OTLP payload generation
- `ScenarioConfig`: Pydantic model for YAML configuration validation
- `Service`, `Database`, `MessageQueue`: Configuration entities
- `Operation`: Business operation modeling for realistic traces
- `LatencyConfig`: Probabilistic performance simulation

#### Frontend Service (React/Vite)
**File Structure**:
```
frontend/src/
├── App.jsx              # Main application component
├── components/
│   ├── Header.jsx       # Application header
│   ├── ScenarioForm.jsx # Natural language input form
│   ├── ConfigDisplay.jsx # YAML configuration display
│   ├── Controls.jsx     # Start/stop telemetry controls
│   └── StatusBar.jsx    # Generator status display
└── main.jsx             # React app entry point
```

**Technology Stack**:
- **React 18**: Component framework
- **Vite**: Build tool and dev server  
- **Tailwind CSS**: Utility-first styling
- **js-yaml**: YAML parsing and validation

#### Telemetry Generation Engine

**Multi-Signal Generation**:
1. **Traces**: Distributed transactions following service dependency graphs
2. **Logs**: Correlated application logs with trace context
3. **Metrics**: System metrics + runtime-specific metrics per language

**Realism Features**:
- **Semantic Conventions**: Proper OTLP attributes (http.method, db.system, etc.)
- **Multi-Language Simulation**: Different telemetry.sdk.language per service
- **Runtime Metrics**: JVM GC (Java), Event Loop (Node.js), Goroutines (Go)
- **Performance Patterns**: Configurable latency, error injection, N+1 queries
- **Resource Attributes**: Cloud provider, regions, host names, versions

**OTLP JSON Payload Structure**:
```json
{
  "resourceSpans": [{
    "resource": {
      "attributes": [
        {"key": "service.name", "value": {"stringValue": "payment-service"}},
        {"key": "telemetry.sdk.language", "value": {"stringValue": "java"}},
        {"key": "cloud.provider", "value": {"stringValue": "aws"}}
      ]
    },
    "scopeSpans": [{
      "scope": {"name": "otel-demo-generator"},
      "spans": [/* OTLP spans */]
    }]
  }]
}
```

## Configuration Schema (YAML)

### Complete Schema Definition
```yaml
services:
  - name: service-name
    language: python|java|nodejs|go|ruby|dotnet|typescript
    role: frontend|backend|database-service|worker
    operations:  # NEW: Business operation modeling
      - name: "ProcessPayment"
        span_name: "POST /payments"
        db_queries:
          - "SELECT * FROM accounts WHERE id = ?"
          - "UPDATE accounts SET balance = ? WHERE id = ?"
        latency:  # NEW: Probabilistic performance simulation
          min_ms: 50
          max_ms: 200
          probability: 0.9
    depends_on:
      - service: other-service
        protocol: http|grpc
        latency:
          min_ms: 10
          max_ms: 50
          probability: 0.05  # 5% slow calls
      - db: database-name
        example_queries:
          - "SELECT * FROM users WHERE email = ?"
        latency:
          min_ms: 5
          max_ms: 500
          probability: 0.02  # 2% slow queries
      - cache: redis-cache
      - queue: kafka-topic

databases:
  - name: postgres-main
    type: postgres|mysql|mongodb|redis

message_queues:
  - name: kafka-broker
    type: kafka|rabbitmq|sqs

telemetry:
  trace_rate: 5           # Traces per second
  error_rate: 0.05        # 5% error rate
  metrics_interval: 10    # Seconds between metric emissions
  include_logs: true
```

### Enhanced Features in Current Implementation

#### Business Operations Modeling
- **Purpose**: Generate realistic business transaction traces
- **Example**: E-commerce checkout with payment processing, inventory checks
- **Benefits**: More meaningful span names, realistic database queries

#### Probabilistic Latency Simulation  
- **Use Cases**: Simulate database slow queries, network latency spikes
- **Configuration**: `probability: 0.1` = 10% of operations have elevated latency
- **Realism**: Models real-world performance variations

#### Example Database Queries
- **PostgreSQL**: `SELECT * FROM orders WHERE user_id = ? AND status = 'pending'`
- **MongoDB**: `db.users.find({email: "user@example.com"})`
- **Redis**: `GET user:session:abc123`

## LLM Integration & System Prompt

### Current OpenAI Integration
- **Model**: o3 model
- **Library**: `openai` Python SDK
- **Authentication**: Environment variable `OPENAI_API_KEY`

### Optimized System Prompt
```
You are an expert assistant that generates architecture configuration files for a microservices observability demo tool.

**Requirements:**
1. Output only YAML code (no explanatory text)
2. Use top-level keys: services, databases, message_queues, telemetry
3. For realistic demos, define operations with business logic
4. Add latency blocks for performance simulation
5. Include example_queries for database dependencies
6. Use variety of languages for polyglot simulation
7. Create coherent dependency relationships

**Performance Simulation Examples:**
- Slow reporting: latency: {min_ms: 1200, max_ms: 3500}
- Intermittent slow DB: latency: {min_ms: 50, max_ms: 800, probability: 0.1}

**Output Format**: Valid YAML only, no ```yaml blocks or explanations
```

### Prompt Engineering Best Practices
- **Clarity**: Explicit output format requirements
- **Examples**: Include realistic latency/operation examples  
- **Constraints**: No prose, only YAML output
- **Flexibility**: Support for various complexity levels

## Deployment & Development

### Local Development Setup
```bash
# Automated startup (recommended)
./start-local.sh

# Manual startup
cd backend && uvicorn main:app --reload --port 8000 &
cd frontend && npm run dev
```

### Environment Variables
```bash
export OPENAI_API_KEY="sk-..."
export OTEL_COLLECTOR_URL="http://localhost:4318"  # Optional
```

### Production Deployment Options

#### Docker Compose (Recommended for testing)
```yaml
services:
  backend:
    build: ./backend
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    ports:
      - "8000:8000"
  
  frontend:
    build: ./frontend
    ports:
      - "5173:5173"
  
  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    ports:
      - "4318:4318"  # OTLP HTTP
```

#### Kubernetes Deployment
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: otel-demo-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app: otel-demo-backend
  template:
    spec:
      containers:
      - name: backend
        image: otel-demo-backend:latest
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: openai-secret
              key: api-key
```

## Technical Implementation Details

### FastAPI Backend Architecture

#### Global State Management
```python
# Global generator instance for singleton pattern
generator_instance: Optional[TelemetryGenerator] = None

# Thread-safe start/stop operations
async def start_generation(request: StartRequest):
    global generator_instance
    if generator_instance and generator_instance.is_running():
        raise HTTPException(status_code=400, detail="Already running")
```

#### CORS Configuration
```python
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:5173", "http://127.0.0.1:5173"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)
```

### Telemetry Generation Engine Details

#### Service Graph Traversal
```python
def _generate_span_recursive(
    self,
    service_name: str,
    parent_span_id: str | None,
    trace_id: str,
    spans_by_service: Dict[str, List[Dict[str, Any]]],
    start_time_ns: int,
    error_source: str | None,
    trigger_kind: str,
    visited_services: set,
    recursion_depth: int
) -> Tuple[int, bool]:
```

#### Runtime-Specific Metrics Generation
```python
# Java JVM metrics
if lang == "java":
    metrics.append(self._create_sum_metric(
        "jvm.gc.collection_count", "collections", True, [...]
    ))

# Go runtime metrics  
elif lang == "go":
    metrics.append(self._create_gauge_metric(
        "go.goroutines", "goroutines", [...]
    ))

# Node.js event loop metrics
elif lang == "nodejs":
    metrics.append(self._create_gauge_metric(
        "nodejs.eventloop.delay.avg", "ms", [...]
    ))
```

#### Resource Attribute Generation
```python
def _generate_resource_attributes(self, service: Service) -> Dict[str, Any]:
    return {
        "service.name": service.name,
        "service.namespace": "otel-demo-gen",
        "service.version": "1.2.3",
        "service.instance.id": f"{service.name}-{secrets.token_hex(6)}",
        "telemetry.sdk.language": service.language,
        "cloud.provider": "aws",
        "cloud.region": "us-west-2",
        "deployment.environment": "production",
        "host.name": f"{service.name}-{secrets.token_hex(2)}",
    }
```

### Frontend React Architecture

#### Component Responsibilities
- **App.jsx**: Main state management, API coordination
- **ScenarioForm.jsx**: Natural language input, form validation  
- **ConfigDisplay.jsx**: YAML syntax highlighting, editing
- **Controls.jsx**: Start/stop buttons, status management
- **StatusBar.jsx**: Real-time generation statistics

#### State Management Pattern
```javascript
const [scenario, setScenario] = useState('');
const [config, setConfig] = useState(null);
const [isGenerating, setIsGenerating] = useState(false);
const [generatorStatus, setGeneratorStatus] = useState({running: false});
```

## Advanced Features & Realism Enhancements

### Error Injection Strategies
1. **Random Service Failures**: HTTP 500 responses, exceptions
2. **Database Timeouts**: Simulated connection failures
3. **Network Latency**: Variable response times
4. **Cascade Failures**: Errors propagating through service chains

### Performance Pattern Simulation
1. **N+1 Database Queries**: Multiple sequential DB calls per request
2. **Memory Leaks**: Gradually increasing memory metrics
3. **GC Pressure**: Elevated garbage collection metrics (Java)
4. **Event Loop Blocking**: High event loop delay (Node.js)

### Semantic Conventions Compliance
- **HTTP Spans**: `http.method`, `http.status_code`, `http.url`
- **Database Spans**: `db.system`, `db.name`, `db.statement`
- **Messaging Spans**: `messaging.system`, `messaging.destination`
- **RPC Spans**: `rpc.service`, `rpc.method`

### Multi-Language Runtime Behaviors
```python
RUNTIME_INFO = {
    "python": {"name": "CPython", "version": "3.11.5"},
    "java": {"name": "OpenJDK Runtime Environment", "version": "17.0.5"},
    "nodejs": {"name": "node.js", "version": "18.12.1"},
    "go": {"name": "go", "version": "1.21.0"},
    "ruby": {"name": "ruby", "version": "3.2.2"},
    "dotnet": {"name": ".NET", "version": "7.0.0"},
}
```

## Extension Points & Future Enhancements

### Planned Features
- **Custom Metric Definitions**: User-defined business metrics
- **Service Mesh Simulation**: Istio/Envoy proxy traces  
- **Time-Based Patterns**: Load spikes, scheduled jobs
- **Topology Visualization**: Interactive service dependency graphs
- **Configuration Templates**: Pre-built scenarios (e-commerce, banking, etc.)

### API Extensions
- **POST /templates**: Get pre-defined scenario templates
- **POST /validate**: Validate YAML configuration
- **GET /metrics**: Generator performance metrics
- **WebSocket /status**: Real-time status streaming

### Integration Opportunities  
- **Grafana Dashboards**: Auto-generated dashboards for scenarios
- **Alert Rule Generation**: Synthetic alert conditions
- **Load Testing Integration**: k6, Artillery scenario generation
- **CI/CD Integration**: Observability testing in pipelines

## Troubleshooting Guide

### Common Issues

#### Generator Won't Start
```bash
# Check OpenAI API key
echo $OPENAI_API_KEY

# Verify collector endpoint
curl http://localhost:4318/v1/traces

# Check backend logs
cd backend && uvicorn main:app --reload --log-level debug
```

#### No Telemetry in Backend
```bash
# Verify OTLP endpoint configuration
curl -X POST http://localhost:8000/status

# Check collector configuration
docker logs otel-collector

# Validate YAML configuration
python -c "import yaml; yaml.safe_load(open('config.yaml'))"
```

#### Frontend Build Issues
```bash
# Clear node modules and reinstall
cd frontend && rm -rf node_modules package-lock.json
npm install

# Update dependencies
npm update
```

### Performance Tuning

#### High CPU Usage
- Reduce `trace_rate` in telemetry configuration  
- Decrease `metrics_interval` frequency
- Lower service count in complex scenarios

#### Memory Issues
- Monitor generator thread lifecycle
- Implement payload batching for high-volume scenarios
- Use connection pooling for OTLP endpoints

## Contributing Guidelines

### Code Structure Standards
- **Backend**: Type hints, docstrings, Pydantic validation
- **Frontend**: Functional components, PropTypes, error boundaries  
- **Configuration**: YAML schema validation, example configurations
- **Testing**: Unit tests for telemetry generation, integration tests for API

### Development Workflow
1. **Feature Branch**: Create from main
2. **Implementation**: Follow existing patterns
3. **Testing**: Verify telemetry output manually
4. **Documentation**: Update architecture guide
5. **Pull Request**: Include example scenarios

This architecture document serves as the definitive reference for understanding, extending, and deploying the AI-Powered Observability Demo Generator. All implementation details are current as of the latest codebase state.
